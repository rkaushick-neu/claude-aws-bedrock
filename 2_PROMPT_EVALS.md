# 2. Prompt Evaluations

Relevant Notebook: [001_Prompt_Evals.ipynb](./notebooks/2-prompt-evaluations/001_Prompt_Evals.ipynb)

To build reliable AI applications, we need to consider **Prompt Engineering** & **Prompt Evaluation**

![Prompt Engineering & Prompt Evaluation](https://everpath-course-content.s3-accelerate.amazonaws.com/instructor%2Fa46l9irobhg0f5webscixp0bs%2Fpublic%2F1748557873%2F06_-_001_-_Prompt_Evaluation_00.1748557873297.png)

After writing a prompt, we must systematically test and evaluate it by running it through an evaluation pipeline. This helps us get an objective score on how well our prompt performs across a range of different scenarios. 

## Prompt Eval Workflow

There are many different ways to evaluate workflows. Here are the steps:

1. **Create an Initial Prompt**
2. **Create an Evaluation Dataset:** Could be 10s/ 100s or 1000s of questions.
3. **Feed Each Question into Claude:** Merge the question into the prompt and feed it through Claude.
4. **Feed Through a Grader & Find the Average:** Evaluates each response with a score. We can take the average score as a measure of overall score for the prompt.
5. **Change Prompt & Repeat**

## Goal

The prompt should take a user's task description and return one of the three outputs:
1. Python code
2. JSON config
3. Regular expressions

### 1. Initial Prompt

```python
prompt_v1 = f"""
Please provide a solution to the following task:

{task}
"""
```

### 2. Evaluation Dataset (Generated by Claude)

```python
prompt = """
    Generate 3 AWS-related tasks that require Python, JSON, or Regex solutions.
    
    Focus on tasks that can be solved by writing a single Python function, 
    a single JSON object, or tasks that do not require writing much code.
    
    Example output:
    [
        {
            "task": "Description of task",
            "format": "json" or "python" or "regex",
            "solution_criteria": "Key criteria to evaluate the solution"
        },
        ...additional
    ]
    
    Please generate 3 objects.
    """
```

Since we want the dataset to strictly be in JSON, we use **message prefilling** & **stop sequences**. The above prompt helped us generate the evaluation dataset: [dataset_v3.json](./evals/dataset_v3.json)

### Step 3: Run the Prompt
We run the initial prompt on the test-cases to get the model output.

### Step 4: Run the Grader
We feed the test-case and the output to the graders.

#### Graders

Three can be three types of graders:

![Graders](https://everpath-course-content.s3-accelerate.amazonaws.com/instructor%2Fa46l9irobhg0f5webscixp0bs%2Fpublic%2F1748557941%2F06_-_005_-_Model_Based_Grading_03.1748557941095.png)


#### Evaluation Criteria
We must clearly define evaluation criteria for a successful grader. For our code generation use-case, we can focus on the following evaluation criteria:

1. Format
2. Valid Syntax
3. Task Following

We can use different graders for each evaluation criteria as shown below:

![Graders for each evaluation criteria](https://everpath-course-content.s3-accelerate.amazonaws.com/instructor%2Fa46l9irobhg0f5webscixp0bs%2Fpublic%2F1748557943%2F06_-_005_-_Model_Based_Grading_07.1748557942738.png)

### Step 5: Run The Evaluation

We loop through each record in the evaluation dataset, and run all the above steps. We can now evaluate the prompt based on the score for each of these test cases. Finally, we iterate and update the prompt to see if it improves the overall score.