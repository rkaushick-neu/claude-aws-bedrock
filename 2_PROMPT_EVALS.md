# 2. Prompt Evaluations

To build reliable AI applications, we need to consider **Prompt Engineering** & **Prompt Evaluation**

![Prompt Engineering & Prompt Evaluation](https://everpath-course-content.s3-accelerate.amazonaws.com/instructor%2Fa46l9irobhg0f5webscixp0bs%2Fpublic%2F1748557873%2F06_-_001_-_Prompt_Evaluation_00.1748557873297.png)

After writing a prompt, we must systematically test and evaluate it by running it through an evaluation pipeline. This helps us get an objective score on how well our prompt performs across a range of different scenarios. 

## Prompt Eval Workflow

There are many different ways to evaluate workflows. Here are the steps:

1. **Create an Initial Prompt**
2. **Create an Evaluation Dataset:** Could be 10s/ 100s or 1000s of questions.
3. **Feed Each Question into Claude:** Merge the question into the prompt adn feed it through Claude.
4. **Feed Through a Grader & Find the Average:** Evaluates each response with a score. We can take the average score as a measure of overall score for the prompt.
5. **Change Prompt & Repeat**

## Goal

The prompt should take a user's tsk description and return one of the three outputs:
1. Python code
2. JSON config
3. Regular expressions

### 1. Initial Prompt

```python
prompt_v1 = f"""
Please provide a solution to the following task:

{task}
"""
```

### 2. Evaluation Dataset (Generated by Claude)

```python
prompt = """
    Generate 3 AWS-related tasks that require Python, JSON, or Regex solutions.
    
    Focus on tasks that can be solved by writing a single Python function, 
    a single JSON object, or tasks that do not require writing much code.
    
    Example output:
    [
        {
            "task": "Description of task"
        },
        ...additional
    ]
    
    Please generate 3 objects.
    """
```

Since we want the dataset to strictly be in JSON, we use **message prefilling** & **stop sequences**. The above prompt helped us generate the evaluation dataset: [dataset.json](./evals/dataset.json)