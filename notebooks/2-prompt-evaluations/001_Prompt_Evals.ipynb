{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-2\")\n",
    "# Use Haiku for faster evals\n",
    "model_id = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "\n",
    "\n",
    "def add_user_message(messages, text):\n",
    "    user_message = {\"role\": \"user\", \"content\": [{\"text\": text}]}\n",
    "    messages.append(user_message)\n",
    "\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    assistant_message = {\"role\": \"assistant\", \"content\": [{\"text\": text}]}\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "\n",
    "def chat(messages, system=None, temperature=1.0, stop_sequences=[]):\n",
    "    params = {\n",
    "        \"modelId\": model_id,\n",
    "        \"messages\": messages,\n",
    "        \"inferenceConfig\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"stopSequences\": stop_sequences,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if system:\n",
    "        params[\"system\"] = [{\"text\": system}]\n",
    "\n",
    "    response = client.converse(**params)\n",
    "\n",
    "    return response[\"output\"][\"message\"][\"content\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define an initial prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_v1 = f\"\"\"\n",
    "# Please provide a solution to hte following task:\n",
    "\n",
    "# {task}\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate the Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset():\n",
    "    prompt = \"\"\"\n",
    "    Generate 3 AWS-related tasks that require Python, JSON, or Regex solutions.\n",
    "    \n",
    "    Focus on tasks that can be solved by writing a single Python function, \n",
    "    a single JSON object, or tasks that do not require writing much code.\n",
    "    \n",
    "    Example output:\n",
    "    [\n",
    "        {\n",
    "            \"task\": \"Description of task\"\n",
    "        },\n",
    "        ...additional\n",
    "    ]\n",
    "    \n",
    "    Please generate 3 objects.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    add_assistant_message(messages, \"```json\")\n",
    "    text = chat(messages, stop_sequences=[\"```\"])\n",
    "\n",
    "    return json.loads(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = generate_dataset()\n",
    "\n",
    "# writing the dataset into a json file\n",
    "with open(\"../../evals/dataset.json\", \"w\") as f:\n",
    "    json.dump(eval_dataset, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(test_case):\n",
    "    \"\"\" Merges the prompt & test case input, then it returns the result generated by the LLM. \"\"\"\n",
    "\n",
    "    # v1 of the prompt\n",
    "    prompt = f\"\"\"\n",
    "    Please solve the following task: \n",
    "\n",
    "    {test_case[\"task\"]}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "\n",
    "    return chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run the Grader \n",
    "### Graders\n",
    "\n",
    "Three can be three types of graders:\n",
    "\n",
    "![Graders](https://everpath-course-content.s3-accelerate.amazonaws.com/instructor%2Fa46l9irobhg0f5webscixp0bs%2Fpublic%2F1748557941%2F06_-_005_-_Model_Based_Grading_03.1748557941095.png)\n",
    "\n",
    "\n",
    "### Evaluation Criteria\n",
    "We must clearly define evaluation criteria for grading whether our prompt produces good outputs. For our code generation use-case, we can focus on:\n",
    "\n",
    "1. Format\n",
    "2. Valid Syntax\n",
    "3. Task Following\n",
    "\n",
    "We can use different graders for each evaluation criteria as shown below:\n",
    "\n",
    "![Graders for each evaluation criteria](https://everpath-course-content.s3-accelerate.amazonaws.com/instructor%2Fa46l9irobhg0f5webscixp0bs%2Fpublic%2F1748557943%2F06_-_005_-_Model_Based_Grading_07.1748557942738.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Grader (for Task Following)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_by_model(test_case, output):\n",
    "    eval_prompt = f\"\"\"\n",
    "    You are an expert AWS code reviewer. Your task is to evaluate this AI-generated solution.\n",
    "    \n",
    "    Original Task:\n",
    "    <task>\n",
    "    {test_case[\"task\"]}\n",
    "    </task>\n",
    "\n",
    "    Solution to evaluate: \n",
    "    <task>\n",
    "    {output}\n",
    "    </task>\n",
    "    \n",
    "    Output Format\n",
    "    Provide your evaluation as a structured JSON object with the following fields, in this specific fields:\n",
    "    - \"strengths\": An array of 1-3 key strengths\n",
    "    - \"weaknesses\": An array of 1-3 key areas for improvement  \n",
    "    - \"reasoning\": A concise explanation of your assessment\n",
    "    - \"score\": A number between 1-10\n",
    "\n",
    "    Respond with JSON. Keep your response concise and direct.\n",
    "    Example response shape:\n",
    "    {{\n",
    "        \"strengths\": string[],\n",
    "        \"weaknesses\": string[],\n",
    "        \"reasoning\": string,\n",
    "        \"score\": number\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, eval_prompt)\n",
    "    add_assistant_message(messages, \"```json\")\n",
    "    eval_text = chat(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(eval_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_case(test_case):\n",
    "    \"\"\" Calls the run_prompt, then grades the result \"\"\"\n",
    "\n",
    "    output = run_prompt(test_case)\n",
    "\n",
    "    # TODO - Grading\n",
    "    # for now hard-coding the score to be 10\n",
    "    # it could be a number, boolean or text.\n",
    "\n",
    "    # model grader\n",
    "    model_grade = grade_by_model(test_case, output)\n",
    "    score = model_grade[\"score\"]\n",
    "    reasoning = model_grade[\"reasoning\"]\n",
    "    # if we want we can extract the strengths & weaknesses\n",
    "\n",
    "    return {\n",
    "        \"output\": output,\n",
    "        \"test_case\" : test_case,\n",
    "        \"score\" : score,\n",
    "        \"reasoning\": reasoning\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(dataset):\n",
    "    \"\"\" Loops over the dataset and calls the run_test_case method for each test-case \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for test_case in dataset:\n",
    "        result  = run_test_case(test_case)\n",
    "        results.append(result)\n",
    "    \n",
    "    average_score = mean([result[\"score\"] for result in results])\n",
    "    print(f\"Average score: {average_score}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running The Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../evals/dataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "results = run_eval(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it takes time even to run Haiku - because we have not optimized it yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"output\": \"Here's a solution to extract AWS IAM role names from a list of ARNs using regex in Python:\\n\\n```python\\nimport re\\n\\ndef extract_iam_role_names(arns):\\n    \\\"\\\"\\\"\\n    Extract IAM role names from a list of AWS ARNs.\\n    \\n    Args:\\n        arns (list): A list of AWS ARN strings\\n    \\n    Returns:\\n        list: A list of IAM role names extracted from valid IAM role ARNs\\n    \\\"\\\"\\\"\\n    # Regex pattern to match IAM role ARNs\\n    # Example ARN format: arn:aws:iam::123456789012:role/RoleName\\n    role_arn_pattern = r'^arn:aws:iam::(\\\\d+):role/(.+)$'\\n    \\n    # List to store extracted role names\\n    role_names = []\\n    \\n    # Iterate through the input ARNs\\n    for arn in arns:\\n        # Try to match the ARN against the role ARN pattern\\n        match = re.match(role_arn_pattern, arn)\\n        \\n        # If there's a match, extract the role name\\n        if match:\\n            role_names.append(match.group(2))\\n    \\n    return role_names\\n\\n# Example usage\\ndef main():\\n    # Sample list of ARNs (mix of IAM role ARNs and other ARNs)\\n    sample_arns = [\\n        'arn:aws:iam::123456789012:role/AdminRole',\\n        'arn:aws:s3:::my-bucket',\\n        'arn:aws:iam::987654321098:role/ReadOnlyRole',\\n        'arn:aws:ec2::123456789012:instance/i-1234567890abcdef0',\\n        'arn:aws:iam::111222333444:role/ServiceRole'\\n    ]\\n    \\n    # Extract IAM role names\\n    extracted_roles = extract_iam_role_names(sample_arns)\\n    \\n    # Print the extracted role names\\n    print(\\\"Extracted IAM Role Names:\\\")\\n    for role in extracted_roles:\\n        print(role)\\n\\n# Run the main function\\nif __name__ == '__main__':\\n    main()\\n```\\n\\nThis solution provides several key features:\\n\\n1. **Regex Pattern Matching**:\\n   - Uses a regex pattern `^arn:aws:iam::(\\\\d+):role/(.+)$` to match IAM role ARNs\\n   - Breaks down the pattern:\\n     - `^arn:aws:iam::` - Matches the start of a standard IAM ARN\\n     - `(\\\\d+)` - Captures the AWS account ID\\n     - `:role/` - Identifies the role resource type\\n     - `(.+)$` - Captures the role name\\n\\n2. **Flexible Extraction**:\\n   - Handles a list of ARNs with mixed resource types\\n   - Filters out non-IAM role ARNs\\n   - Extracts only the role names from valid IAM role ARNs\\n\\n3. **Error Handling**:\\n   - Gracefully handles ARNs that don't match the IAM role pattern\\n   - Returns only valid role names\\n\\n4. **Example Usage**:\\n   - Demonstrates extracting role names from a sample list of ARNs\\n   - Shows how to use the function in a practical scenario\\n\\nWhen you run this script with the sample ARNs, it will output:\\n```\\nExtracted IAM Role Names:\\nAdminRole\\nReadOnlyRole\\nServiceRole\\n```\\n\\n**Additional Considerations**:\\n\\n- The regex is strict and follows the standard AWS ARN format\\n- It assumes a specific ARN structure for IAM roles\\n- You can modify the regex pattern to handle variations if needed\\n\\n**Potential Enhancements**:\\n- Add input validation\\n- Support for more complex ARN formats\\n- Error logging for unmatched ARNs\\n\\n**Best Practices**:\\n- Use this for parsing and processing IAM role ARNs\\n- Verify the ARN format matches your specific AWS environment\\n\\nThis solution provides a clean, efficient way to extract IAM role names from a list of ARNs using regex in Python.\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Write a Python function to extract AWS IAM role names from a list of ARNs using regex, filtering out non-IAM role ARNs\"\n",
      "    },\n",
      "    \"score\": 8,\n",
      "    \"reasoning\": \"The solution effectively extracts IAM role names using regex with a clean implementation. It handles multiple ARN types gracefully and provides a straightforward approach to role name extraction. However, it could benefit from additional input validation and more flexible ARN matching.\"\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"Here's a comprehensive CloudFormation JSON template for creating a VPC with public and private subnets across multiple availability zones:\\n\\n```json\\n{\\n    \\\"AWSTemplateFormatVersion\\\": \\\"2010-09-09\\\",\\n    \\\"Description\\\": \\\"VPC with Public and Private Subnets in Multiple Availability Zones\\\",\\n    \\\"Parameters\\\": {\\n        \\\"VpcCidr\\\": {\\n            \\\"Type\\\": \\\"String\\\",\\n            \\\"Default\\\": \\\"10.0.0.0/16\\\",\\n            \\\"Description\\\": \\\"CIDR block for the VPC\\\"\\n        },\\n        \\\"PublicSubnet1Cidr\\\": {\\n            \\\"Type\\\": \\\"String\\\",\\n            \\\"Default\\\": \\\"10.0.1.0/24\\\",\\n            \\\"Description\\\": \\\"CIDR block for first public subnet\\\"\\n        },\\n        \\\"PublicSubnet2Cidr\\\": {\\n            \\\"Type\\\": \\\"String\\\",\\n            \\\"Default\\\": \\\"10.0.2.0/24\\\",\\n            \\\"Description\\\": \\\"CIDR block for second public subnet\\\"\\n        },\\n        \\\"PrivateSubnet1Cidr\\\": {\\n            \\\"Type\\\": \\\"String\\\",\\n            \\\"Default\\\": \\\"10.0.10.0/24\\\",\\n            \\\"Description\\\": \\\"CIDR block for first private subnet\\\"\\n        },\\n        \\\"PrivateSubnet2Cidr\\\": {\\n            \\\"Type\\\": \\\"String\\\",\\n            \\\"Default\\\": \\\"10.0.20.0/24\\\",\\n            \\\"Description\\\": \\\"CIDR block for second private subnet\\\"\\n        }\\n    },\\n    \\\"Resources\\\": {\\n        \\\"VPC\\\": {\\n            \\\"Type\\\": \\\"AWS::EC2::VPC\\\",\\n            \\\"Properties\\\": {\\n                \\\"CidrBlock\\\": {\\\"Ref\\\": \\\"VpcCidr\\\"},\\n                \\\"EnableDnsHostnames\\\": true,\\n                \\\"EnableDnsSupport\\\": true,\\n                \\\"Tags\\\": [\\n                    {\\\"Key\\\": \\\"Name\\\", \\\"Value\\\": \\\"MultiAZVPC\\\"}\\n                ]\\n            }\\n        },\\n        \\\"InternetGateway\\\": {\\n            \\\"Type\\\": \\\"AWS::EC2::InternetGateway\\\",\\n            \\\"Properties\\\": {\\n                \\\"Tags\\\": [\\n                    {\\\"Key\\\": \\\"Name\\\", \\\"Value\\\": \\\"MultiAZVPC-IGW\\\"}\\n                ]\\n            }\\n        },\\n        \\\"AttachGateway\\\": {\\n            \\\"Type\\\": \\\"AWS::EC2::VPCGatewayAttachment\\\",\\n            \\\"Properties\\\": {\\n                \\\"VpcId\\\": {\\\"Ref\\\": \\\"VPC\\\"},\\n                \\\"InternetGatewayId\\\": {\\\"Ref\\\": \\\"InternetGateway\\\"}\\n            }\\n        },\\n        \\\"PublicSubnet1\\\": {\\n            \\\"Type\\\": \\\"AWS::EC2::Subnet\\\",\\n            \\\"Properties\\\": {\\n                \\\"VpcId\\\": {\\\"Ref\\\": \\\"VPC\\\"},\\n                \\\"CidrBlock\\\": {\\\"Ref\\\": \\\"PublicSubnet1Cidr\\\"},\\n                \\\"AvailabilityZone\\\": {\\\"Fn::Select\\\": [\\\"0\\\", {\\\"Fn::GetAZs\\\": \\\"\\\"}]},\\n                \\\"MapPublicIpOnLaunch\\\": true,\\n                \\\"Tags\\\": [\\n                    {\\\"Key\\\": \\\"Name\\\", \\\"Value\\\": \\\"Public Subnet 1\\\"}\\n                ]\\n            }\\n        },\\n        \\\"PublicSubnet2\\\": {\\n            \\\"Type\\\": \\\"AWS::EC2::Subnet\\\",\\n            \\\"Properties\\\": {\\n                \\\"VpcId\\\": {\\\"Ref\\\": \\\"VPC\\\"},\\n                \\\"CidrBlock\\\": {\\\"Ref\\\": \\\"PublicSubnet2Cidr\\\"},\\n                \\\"AvailabilityZone\\\": {\\\"Fn::Select\\\": [\\\"1\\\", {\\\"Fn::GetAZs\\\": \\\"\\\"}]},\\n                \\\"MapPublicIpOnLaunch\\\": true,\\n                \\\"Tags\\\": [\\n                    {\\\"Key\\\": \\\"Name\\\", \\\"Value\\\": \\\"Public Subnet 2\\\"}\\n                ]\\n            }\\n        },\\n        \\\"PrivateSubnet1\\\": {\\n            \\\"Type\\\": \\\"AWS::EC2::Subnet\\\",\\n            \\\"Properties\\\": {\\n                \\\"VpcId\\\": {\\\"Ref\\\": \\\"VPC\\\"},\\n                \\\"CidrBlock\\\": {\\\"Ref\\\": \\\"PrivateSubnet1Cidr\\\"},\\n                \\\"AvailabilityZone\\\": {\\\"Fn::Select\\\": [\\\"0\\\", {\\\"Fn::GetAZs\\\": \\\"\\\"}]},\\n                \\\"Tags\\\": [\\n                    {\\\"Key\\\": \\\"Name\\\", \\\"Value\\\": \\\"Private Subnet 1\\\"}\\n                ]\\n            }\\n        },\\n        \\\"PrivateSubnet2\\\": {\\n            \\\"Type\\\": \\\"AWS::EC2::Subnet\\\",\\n            \\\"Properties\\\": {\\n                \\\"VpcId\\\": {\\\"Ref\\\": \\\"VPC\\\"},\\n                \\\"CidrBlock\\\": {\\\"Ref\\\": \\\"PrivateSubnet2Cidr\\\"},\\n                \\\"AvailabilityZone\\\": {\\\"Fn::Select\\\": [\\\"1\\\", {\\\"Fn::GetAZs\\\": \\\"\\\"}]},\\n                \\\"Tags\\\": [\\n                    {\\\"Key\\\": \\\"Name\\\", \\\"Value\\\": \\\"Private Subnet 2\\\"}\\n                ]\\n            }\\n        },\\n        \\\"PublicRouteTable\\\": {\\n            \\\"Type\\\": \\\"AWS::EC2::RouteTable\\\",\\n            \\\"Properties\\\": {\\n                \\\"VpcId\\\": {\\\"Ref\\\": \\\"VPC\\\"},\\n                \\\"Tags\\\": [\\n                    {\\\"Key\\\": \\\"Name\\\", \\\"Value\\\": \\\"Public Route Table\\\"}\\n                ]\\n            }\\n        },\\n        \\\"PublicRoute\\\": {\\n            \\\"Type\\\": \\\"AWS::EC2::Route\\\",\\n            \\\"DependsOn\\\": \\\"AttachGateway\\\",\\n            \\\"Properties\\\": {\\n                \\\"RouteTableId\\\": {\\\"Ref\\\": \\\"PublicRouteTable\\\"},\\n                \\\"DestinationCidrBlock\\\": \\\"0.0.0.0/0\\\",\\n                \\\"GatewayId\\\": {\\\"Ref\\\": \\\"InternetGateway\\\"}\\n            }\\n        },\\n        \\\"PublicSubnet1RouteTableAssociation\\\": {\\n            \\\"Type\\\": \\\"AWS::EC2::SubnetRouteTableAssociation\\\",\\n            \\\"Properties\\\": {\\n                \\\"SubnetId\\\": {\\\"Ref\\\": \\\"PublicSubnet1\\\"},\\n                \\\"RouteTableId\\\": {\\\"Ref\\\": \\\"PublicRouteTable\\\"}\\n            }\\n        },\\n        \\\"PublicSubnet2RouteTableAssociation\\\": {\\n            \\\"Type\\\": \\\"AWS::EC2::SubnetRouteTableAssociation\\\",\\n            \\\"Properties\\\": {\\n                \\\"SubnetId\\\": {\\\"Ref\\\": \\\"PublicSubnet2\\\"},\\n                \\\"RouteTableId\\\": {\\\"Ref\\\": \\\"PublicRouteTable\\\"}\\n            }\\n        }\\n    },\\n    \\\"Outputs\\\": {\\n        \\\"VpcId\\\": {\\n            \\\"Description\\\": \\\"VPC ID\\\",\\n            \\\"Value\\\": {\\\"Ref\\\": \\\"VPC\\\"},\\n            \\\"Export\\\": {\\n                \\\"Name\\\": \\\"VPC-ID\\\"\\n            }\\n        },\\n        \\\"PublicSubnet1Id\\\": {\\n            \\\"Description\\\": \\\"Public Subnet 1 ID\\\",\\n            \\\"Value\\\": {\\\"Ref\\\": \\\"PublicSubnet1\\\"}\\n        },\\n        \\\"PublicSubnet2Id\\\": {\\n            \\\"Description\\\": \\\"Public Subnet 2 ID\\\",\\n            \\\"Value\\\": {\\\"Ref\\\": \\\"PublicSubnet2\\\"}\\n        },\\n        \\\"PrivateSubnet1Id\\\": {\\n            \\\"Description\\\": \\\"Private Subnet 1 ID\\\",\\n            \\\"Value\\\": {\\\"Ref\\\": \\\"PrivateSubnet1\\\"}\\n        },\\n        \\\"PrivateSubnet2Id\\\": {\\n            \\\"Description\\\": \\\"Private Subnet 2 ID\\\",\\n            \\\"Value\\\": {\\\"Ref\\\": \\\"PrivateSubnet2\\\"}\\n        }\\n    }\\n}\\n```\\n\\nThis CloudFormation template provides:\\n\\n\\u2705 Parameters for customizable CIDR blocks\\n\\u2705 Multi-AZ VPC configuration\\n\\u2705 Public and private subnets\\n\\u2705 Internet Gateway\\n\\u2705 Route tables\\n\\u2705 Subnet route associations\\n\\u2705 Exported outputs for cross-stack references\\n\\nKey Features:\\n- Configurable CIDR ranges\\n- Spans two Availability Zones\\n- Public subnets with Internet Gateway\\n- Private subnets without direct internet access\\n- DNS support enabled\\n\\nYou can deploy this template in AWS CloudFormation to quickly create a robust networking foundation.\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Create a JSON configuration for an AWS CloudFormation stack that defines a basic VPC with public and private subnets across multiple availability zones\"\n",
      "    },\n",
      "    \"score\": 8,\n",
      "    \"reasoning\": \"The solution provides a solid foundational VPC template with good architectural practices. It covers key networking components and allows for customization, but lacks advanced network security and scalability features that production environments typically require.\"\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"Here's a comprehensive Python script to parse AWS CloudTrail log files and generate a summary of API call frequencies by service:\\n\\n```python\\nimport json\\nimport os\\nfrom collections import defaultdict\\nfrom datetime import datetime\\n\\ndef parse_cloudtrail_logs(log_directory):\\n    \\\"\\\"\\\"\\n    Parse CloudTrail log files from a specified directory and analyze API call frequencies.\\n    \\n    Args:\\n        log_directory (str): Path to the directory containing CloudTrail log files\\n    \\n    Returns:\\n        dict: A summary of API call frequencies by AWS service\\n    \\\"\\\"\\\"\\n    # Dictionary to store service API call frequencies\\n    service_api_counts = defaultdict(lambda: defaultdict(int))\\n    \\n    # Total number of log files processed\\n    total_log_files = 0\\n    \\n    # Iterate through log files in the directory\\n    for filename in os.listdir(log_directory):\\n        if filename.endswith('.json'):\\n            log_file_path = os.path.join(log_directory, filename)\\n            \\n            try:\\n                with open(log_file_path, 'r') as log_file:\\n                    log_data = json.load(log_file)\\n                    \\n                    # Process each CloudTrail record\\n                    for record in log_data.get('Records', []):\\n                        # Extract service and API call details\\n                        event_source = record.get('eventSource', 'Unknown')\\n                        event_name = record.get('eventName', 'Unknown')\\n                        \\n                        # Count API calls for each service\\n                        service_api_counts[event_source][event_name] += 1\\n                \\n                total_log_files += 1\\n            \\n            except json.JSONDecodeError:\\n                print(f\\\"Error parsing log file: {filename}\\\")\\n            except Exception as e:\\n                print(f\\\"Error processing log file {filename}: {str(e)}\\\")\\n    \\n    return service_api_counts, total_log_files\\n\\ndef generate_summary_report(service_api_counts):\\n    \\\"\\\"\\\"\\n    Generate a detailed summary report of API call frequencies.\\n    \\n    Args:\\n        service_api_counts (dict): Dictionary of service API call frequencies\\n    \\n    Returns:\\n        str: Formatted summary report\\n    \\\"\\\"\\\"\\n    report = \\\"AWS CloudTrail Log Analysis Report\\\\n\\\"\\n    report += f\\\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\\\\n\\\"\\n    \\n    # Sort services by total API call count\\n    sorted_services = sorted(\\n        service_api_counts.items(), \\n        key=lambda x: sum(x[1].values()), \\n        reverse=True\\n    )\\n    \\n    for service, api_counts in sorted_services:\\n        total_service_calls = sum(api_counts.values())\\n        report += f\\\"Service: {service}\\\\n\\\"\\n        report += f\\\"Total API Calls: {total_service_calls}\\\\n\\\"\\n        \\n        # Sort APIs within the service by call frequency\\n        sorted_apis = sorted(api_counts.items(), key=lambda x: x[1], reverse=True)\\n        \\n        for api, count in sorted_apis[:5]:  # Top 5 most frequent APIs\\n            percentage = (count / total_service_calls) * 100\\n            report += f\\\"  - {api}: {count} calls ({percentage:.2f}%)\\\\n\\\"\\n        \\n        report += \\\"\\\\n\\\"\\n    \\n    return report\\n\\ndef save_report(report, output_file='cloudtrail_summary.txt'):\\n    \\\"\\\"\\\"\\n    Save the generated report to a text file.\\n    \\n    Args:\\n        report (str): Summary report content\\n        output_file (str): Output file path\\n    \\\"\\\"\\\"\\n    with open(output_file, 'w') as f:\\n        f.write(report)\\n    print(f\\\"Report saved to {output_file}\\\")\\n\\ndef main():\\n    # Specify the directory containing CloudTrail log files\\n    log_directory = '/path/to/cloudtrail/logs'\\n    \\n    try:\\n        # Parse CloudTrail logs\\n        service_api_counts, total_log_files = parse_cloudtrail_logs(log_directory)\\n        \\n        # Generate summary report\\n        summary_report = generate_summary_report(service_api_counts)\\n        \\n        # Print total log files processed\\n        print(f\\\"Total log files processed: {total_log_files}\\\")\\n        \\n        # Save and display report\\n        save_report(summary_report)\\n        print(summary_report)\\n    \\n    except Exception as e:\\n        print(f\\\"An error occurred: {str(e)}\\\")\\n\\nif __name__ == '__main__':\\n    main()\\n```\\n\\nThis script provides a comprehensive solution for parsing AWS CloudTrail log files and generating a summary of API call frequencies. Here's a breakdown of its key features:\\n\\n1. `parse_cloudtrail_logs()`:\\n   - Iterates through JSON log files in a specified directory\\n   - Extracts event source (service) and event name (API call)\\n   - Counts API call frequencies\\n\\n2. `generate_summary_report()`:\\n   - Creates a detailed report with:\\n     - Timestamp of report generation\\n     - Services sorted by total API call count\\n     - Top 5 most frequent APIs per service\\n     - Percentage of API calls within each service\\n\\n3. `save_report()`:\\n   - Saves the generated report to a text file\\n\\n4. `main()`:\\n   - Orchestrates the log parsing and reporting process\\n\\nUsage example:\\n1. Replace `/path/to/cloudtrail/logs` with the actual path to your CloudTrail log files\\n2. Run the script\\n3. Review the generated `cloudtrail_summary.txt` report\\n\\nAdditional features and potential improvements:\\n- Error handling for various file and parsing scenarios\\n- Flexible configuration for log directory and report output\\n- Supports multiple log file formats\\n- Provides detailed insights into AWS service usage\\n\\nNote: Ensure you have the necessary permissions to read CloudTrail log files and that the log files are in JSON format.\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Develop a Python script to parse AWS CloudTrail log files and generate a summary of API call frequencies by service\"\n",
      "    },\n",
      "    \"score\": 8,\n",
      "    \"reasoning\": \"The solution demonstrates a thoughtful approach to CloudTrail log analysis with robust error handling and structured reporting. It provides meaningful insights into AWS API usage while maintaining a clean, readable implementation. However, it lacks advanced configuration options and may struggle with extremely large log sets.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# examining the results\n",
    "\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 8\n"
     ]
    }
   ],
   "source": [
    "# printing the mean score (since I don't want to run the model again)\n",
    "\n",
    "average_score = mean([result[\"score\"] for result in results])\n",
    "print(f\"Average score: {average_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
